{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chap4-The-Cross-Entroy-Method\n",
    "\n",
    "关于cross-entropy方法有以下两个特点：\n",
    "* 简单，利用pytorch实现的代码行数不超过100行\n",
    "* 这种方法具有很好的收敛性。对于简单的环境中，不需要复杂，多步骤的策略来学习和探索，在较短的周期内就可以获得频繁的反馈，cross-entropy方法通常效果比较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The taxonomy of RL methods\n",
    "\n",
    "cross-entropy 方法是免模型(model-free)和基于政策(policy-based)的方法。\n",
    "\n",
    "* 免模型：意味着这种方法不会建立一个关于环境和奖励的模型，而是直接对现存的观测进行计算得到需要采取的动作。\n",
    "* 基于策略：基于策略的方法可以近似看作是代理的策略，即代理在每一步应该执行什么操作，策略可以用操作的概念分布表示\n",
    "\n",
    "所以cross-entropy方法是model-free，policy-based和on-policy的方法：\n",
    "* 不需要构建环境模型，直接告诉代理如何在下一步采取动作\n",
    "* 近似代理的策略\n",
    "* 需要从环境中获取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross-entropy method in pratice\n",
    "\n",
    "在代理的活动周期，其对应的经历可以用一个周期(episode)表示，每一个周期(episode)是代理在环境中获得的观测，采取的动作以及从采取的动作获得奖励的集合。 \n",
    "\n",
    "由于环境和代理采取动作的随机性，一些活动周期比另一些要好一些。cross-entropy方法的核心是将一些表现差的活动周期扔掉然后在表现好的活动周期上训练，所以，该方法的流程如下：\n",
    "* 使用现有的模型和环境运行N次周期\n",
    "* 计算每个周期的奖励，然后设定一个奖励边界，通常使用的是所有奖励分数的百分数，例如50%或者70%\n",
    "* 丢弃所有奖励分数低于奖励边界的周期\n",
    "* 在保留**精英**episode上进行训练，将观测作为输入，对应的动作作为想要的输出\n",
    "* 从第一步开始重复直到结果令人满意\n",
    "\n",
    "> 在上述步骤进行的过程中，神经网络学习如何重复进行动作来获得更大的奖励，然后将奖励分数变得越来越高。尽管这个算法很简单，但是它在基本的环境中运行效果很好，很容易实施，调整超参数的时候也非常稳定，让它成为一个理性的baseline。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross-entropy method on CartPole\n",
    "\n",
    "模型的核心是一个单隐藏层的神经网络，包含ReLU层和128个隐藏层神经元，该方法比较稳定并且收敛的速度非常快，所以设定的迭代次数较少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import basic modules\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 hidden size neural\n",
    "\n",
    "这个神经网络没有比较特殊的地方，它知识从环境中获取输入向量，然后输出我们可以采取的每一个动作的数字。神经网络的输出是表示动作的分布概率，所以最简单的处理方法就是在最后一层包含一个softmax非线性层。但是在处理神经网络的时候，不需要采取一个softmax层来增加训练过程中的数值稳定性。\n",
    "\n",
    "这里可以直接采用`PyTorch`的类`nn.CrossEntropyLoss`来实现交叉熵损失计算，它包含了`softmax`和交叉熵计算，并且数字表达方式更加稳定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EpsideStep and Episode\n",
    "\n",
    "定义了两个类来保存迭代过程的数据：\n",
    "* EpisodeStep:表示代理在每一次迭代时采取的动作，保存了代理在环境中获取的观测以及采取的动作。最后使用精英`elite`的周期作为训练数据\n",
    "* Episode:单个迭代周期存储在Episode中，对应的键值是该迭代周期总共的奖励分数\n",
    "\n",
    "然后是在episode周期生成的训练用的batch：\n",
    "在该函数中需要输入环境`env`，模型`net`以及需要生成多少步骤的`episode`。在每一次迭代的时候，将当前的观测转换为`PyTorch`向量`tensor`，然后将它传递给神经网络获取动作概率。以下是需要注意的几点：\n",
    "* 在`PyTorch`中所有的`nn.module`实例期望获取的是一个数据实体的`batch`，当然对于这里定义的神经网络也是如此，所以需要将观测的数据转换为`1x4`的`tensor`\n",
    "* 因为在神经网络的输出中没有采用非线性层，所以神经网络输出的是原始的动作分数，在神经网络的输出后再应用上`softmax`函数\n",
    "* 神经网络和`softmax`层都返回了包含梯度的`tensor`，所以需要通过访问`torch.data`域来解包，然后将其转换为`NumPy`数组。该数组有相同的二维结构，我们只需要获取第一维的数据以获得动作概率。\n",
    "\n",
    "现在有了动作的分布概率，我们可以通过`numpy`库的`random.choice()`来采样分布概率获取实际采取的动作。然后，将动作实施到环境中获取下一步的`observation`、`reward`以及周期是否结束的标志`is_done`\n",
    "\n",
    "然后将奖励添加到当前的活动周期的总奖励分数中，`episode`同样用`(observation, action)`来表示。注意当前对应的`observation`是用来选择`action`的观测，而不是`action`采取之后环境变化之后的`observation`。虽然区别比较细微，但是还是需要注意这一点。\n",
    "\n",
    "然后就是处理当前事件结束的情况。我们将完成的事件添加到`batch`，然后保存总的奖励分数和采取的步骤。然后重置总奖励累加变量，清除步骤保存列表，重新设置环境，再次开始。\n",
    "\n",
    "> 另一个比较重要的点是在生成事件的同时，神经网络同时也在进行训练。在每一次迭代生成足够的事件`episode`的同时，我们也在利用梯度下降训练神经网络。所以当`yield`返回生成的数据的时候，神经网络会变得不同，朝着期望的方向变得稍微更好一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batches with episodes\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        if is_done:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter batch\n",
    "\n",
    "这个函数是`cross-entropy`方法的核心，从给定一批的事件和百分数中计算奖励边界，用来筛选用来训练的精英事件。使用`np.percentile`函数获取奖励边界，从一系列的值中和希望的百分数中，计算百分位数。然后计算一个平均奖励分数，这个数只是用来观察。\n",
    "\n",
    "然后对于该批次的每个事件，检测总的奖励分数是否高于奖励边界，如果高于就将其观测和动作添加到训练集中。\n",
    "\n",
    "最后需要将选择出来的精英奖励和动作转换为`tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, steps))\n",
    "        train_act.extend(map(lambda step: step.action, steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "\n",
    "然后将所有的代码添加在一起，主要是包含训练迭代的训练\n",
    "\n",
    "在这部分代码的一开始，创建了所有需要的对象：环境模型、神经网络、目标函数、优化器以及用于观察训练效果的`Tensorboard.SummaryWriter`\n",
    "\n",
    "在训练过程中，迭代生成批数据，然后使用`filter_batch`函数过滤得到`elite`事件。其中的`reward_b`和`reward_m`是用来过滤的奖励边界和观测的奖励均值。之后将神经网络的梯度归零，将观测输入神经网络，获得其对应的动作分数。该动作分数传给目标函数，该目标函数在神经网络输出和代理采取的动作中计算cross-entropy损失。这个思想就是对神经网络强化学习，让其学习这些精英的动作来获得更高的分数。然后我们在交叉熵损失中计算梯度并让优化器调整神经网络。\n",
    "\n",
    "> 可以看到，上述的方法中神经网络学习的是如何在环境中获得的观测中采取动作，不需要和环境进行任何其他的交互。所以这种方法的实施不太依赖于环境的细节。这就是强化学习模型的魅力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.conda\\envs\\rlbook\\lib\\site-packages\\gym\\envs\\registration.py:506: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  f\"The environment {path} is out of date. You should consider \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.697, reward_mean=22.3, rw_bound=26.0\n",
      "1: loss=0.674, reward_mean=28.1, rw_bound=30.0\n",
      "2: loss=0.661, reward_mean=31.8, rw_bound=37.5\n",
      "3: loss=0.660, reward_mean=36.0, rw_bound=35.0\n",
      "4: loss=0.632, reward_mean=39.3, rw_bound=43.5\n",
      "5: loss=0.612, reward_mean=44.5, rw_bound=49.5\n",
      "6: loss=0.612, reward_mean=45.4, rw_bound=57.5\n",
      "7: loss=0.610, reward_mean=49.8, rw_bound=64.5\n",
      "8: loss=0.604, reward_mean=45.0, rw_bound=50.0\n",
      "9: loss=0.573, reward_mean=42.8, rw_bound=55.0\n",
      "10: loss=0.587, reward_mean=66.2, rw_bound=65.0\n",
      "11: loss=0.593, reward_mean=66.1, rw_bound=88.5\n",
      "12: loss=0.570, reward_mean=69.5, rw_bound=86.0\n",
      "13: loss=0.558, reward_mean=75.4, rw_bound=87.5\n",
      "14: loss=0.577, reward_mean=63.2, rw_bound=68.5\n",
      "15: loss=0.542, reward_mean=71.8, rw_bound=82.0\n",
      "16: loss=0.554, reward_mean=78.0, rw_bound=86.0\n",
      "17: loss=0.561, reward_mean=82.6, rw_bound=101.0\n",
      "18: loss=0.569, reward_mean=55.1, rw_bound=60.5\n",
      "19: loss=0.553, reward_mean=79.1, rw_bound=83.0\n",
      "20: loss=0.551, reward_mean=82.9, rw_bound=94.0\n",
      "21: loss=0.528, reward_mean=68.9, rw_bound=87.5\n",
      "22: loss=0.541, reward_mean=105.8, rw_bound=113.5\n",
      "23: loss=0.534, reward_mean=105.4, rw_bound=114.0\n",
      "24: loss=0.542, reward_mean=91.8, rw_bound=107.0\n",
      "25: loss=0.525, reward_mean=101.2, rw_bound=126.5\n",
      "26: loss=0.554, reward_mean=94.1, rw_bound=100.0\n",
      "27: loss=0.536, reward_mean=114.0, rw_bound=123.5\n",
      "28: loss=0.525, reward_mean=121.6, rw_bound=135.5\n",
      "29: loss=0.525, reward_mean=139.4, rw_bound=166.0\n",
      "30: loss=0.544, reward_mean=129.3, rw_bound=156.5\n",
      "31: loss=0.522, reward_mean=156.6, rw_bound=193.0\n",
      "32: loss=0.531, reward_mean=144.9, rw_bound=153.5\n",
      "33: loss=0.518, reward_mean=141.1, rw_bound=162.5\n",
      "34: loss=0.540, reward_mean=151.0, rw_bound=181.0\n",
      "35: loss=0.510, reward_mean=155.2, rw_bound=183.5\n",
      "36: loss=0.529, reward_mean=151.3, rw_bound=156.5\n",
      "37: loss=0.526, reward_mean=178.2, rw_bound=200.0\n",
      "38: loss=0.526, reward_mean=175.2, rw_bound=200.0\n",
      "39: loss=0.522, reward_mean=182.2, rw_bound=200.0\n",
      "40: loss=0.531, reward_mean=172.8, rw_bound=200.0\n",
      "41: loss=0.521, reward_mean=181.7, rw_bound=200.0\n",
      "42: loss=0.529, reward_mean=191.2, rw_bound=200.0\n",
      "43: loss=0.527, reward_mean=184.2, rw_bound=200.0\n",
      "44: loss=0.521, reward_mean=186.6, rw_bound=200.0\n",
      "45: loss=0.538, reward_mean=200.0, rw_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-cartpole\")\n",
    "    \n",
    "    for iter_no, batch in enumerate(iterate_batches(\n",
    "            env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = \\\n",
    "            filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 199:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "import time\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "obs = env.reset()\n",
    "# env.render()\n",
    "sm = nn.Softmax(dim=1)\n",
    "total_reward = 0.0\n",
    "while True:\n",
    "    obs_v = torch.FloatTensor([obs])\n",
    "    act_probs_v = sm(net(obs_v))\n",
    "    act_probs = act_probs_v.data.numpy()[0]\n",
    "    action = np.random.choice(len(act_probs), p=act_probs)\n",
    "    next_obs, reward, is_done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    # time.sleep(1)\n",
    "    if is_done:\n",
    "        print(total_reward)\n",
    "        break\n",
    "    obs = next_obs\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross-entropy method on FrozenLake\n",
    "\n",
    "这个环境是`FrozenLake`，是一个所谓的网格世界分类，你的`agent`生活在一个4x4大小的网格中，只能朝着四个方向进行移动，上、下、左和右。`agent`的起点是左上角，目标是达到右底部。在某些特定的网格中存在陷阱，如果你掉入这些陷阱的话，该次事件结束，并且奖励为零。如果`agent`到达目标网格，获得1.0的奖励分数并且事件结束\n",
    "\n",
    "为了让它变得复杂一点，可以定义这个世界的环境比较滑，毕竟是一个冰雪世界。所以`agent`采取的动作并不是和期望的一致-它有33%的概率可能会滑向右边或者左边。例如，如果你想要代理移动到左边，有33%的概率确实会移动到左边，另外33%的概率可能会在网格之前停下，最后33%概率可能会在网格之后停下。这样使得过程变得稍微艰难一些。\n",
    "\n",
    "环境的观测空间是离散的，也就是包含0到15的数字，即当前在网格中的位置。动作空间同样是离散的，但是它可以从0到3。神经网络期望的是数值向量，我们可以采用传统的one-hot编码离散的输入。\n",
    "\n",
    "但是直接采用`cross-entropy`方法会导致无法收敛，主要是因为：\n",
    "* 对于训练来说，事件执行的步数需要是有限的，并且恰到好处的短\n",
    "* 事件的总的奖励需要有足够的辨识度可以分辨好事件和坏事件\n",
    "* 没有立即表示`agent`是否成功或者失败\n",
    "\n",
    "对于本问题的解决方法，有以下几个：\n",
    "* 更多的数据：对于`CartPole`环境，每次迭代16个事件已经足够训练了，但是对于`FrozenLake`需要至少100次才能得到成功的事件\n",
    "* 对于奖励设置一个衰减因子：让总的奖励分数和事件的长度相关，对于总的奖励分数设置一个衰减因子从0.9到0.95。对于短的事件有较高的奖励分数，这样增加了奖励分布的变化情况，避免出现无法分开精英事件的情况。\n",
    "* 减少学习率：让神经网络有更多的时间来平均更多的训练样本\n",
    "* 保留精英事件一段事件：在之前的训练过程中，从环境中采样好的事件用以训练，然后就将其丢弃了。在`FrozenLake`中，成功的事件比较少见，所以可以考虑将其保存用以多次迭代训练\n",
    "* 更长的训练时间：因为成功时间的稀疏性，以及动作的随机性，神经网络在特定的情况下很难决定采取何种行动。为了达到50%的准确率，至少需要5k次迭代训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "e.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic modules\n",
    "import random\n",
    "import gym, gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space,\n",
    "                          gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n, )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            0.0, 1.0, shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    filter_fun = lambda s: s.reward * (GAMMA ** len(s.steps))\n",
    "    disc_rewards = list(map(filter_fun, batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation,\n",
    "                                 example.steps))\n",
    "            train_act.extend(map(lambda step: step.action,\n",
    "                                 example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    random.seed(12345)\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-tweaked\")\n",
    "\n",
    "    full_batch = []\n",
    "    for iter_no, batch in enumerate(iterate_batches(\n",
    "            env, net, BATCH_SIZE)):\n",
    "        reward_mean = float(np.mean(list(map(\n",
    "            lambda s: s.reward, batch))))\n",
    "        full_batch, obs, acts, reward_bound = \\\n",
    "            filter_batch(full_batch + batch, PERCENTILE)\n",
    "        if not full_batch:\n",
    "            continue\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        acts_v = torch.LongTensor(acts)\n",
    "        full_batch = full_batch[-500:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, rw_mean=%.3f, \"\n",
    "              \"rw_bound=%.3f, batch=%d\" % (\n",
    "            iter_no, loss_v.item(), reward_mean,\n",
    "            reward_bound, len(full_batch)))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "        if reward_mean > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The theoratical background of the cross-entropy method\n",
    "\n",
    "[cross-entropy method simple intro](https://people.smp.uq.edu.au/DirkKroese/ps/eormsCE.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c08bec6e8349631744bf8012680f4ed9f87ad05429dee36fcc42750dcdea16a4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('rlbook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
