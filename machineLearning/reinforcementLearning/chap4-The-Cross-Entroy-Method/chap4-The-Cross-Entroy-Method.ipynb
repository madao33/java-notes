{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chap4-The-Cross-Entroy-Method\n",
    "\n",
    "关于cross-entropy方法有以下两个特点：\n",
    "* 简单，利用pytorch实现的代码行数不超过100行\n",
    "* 这种方法具有很好的收敛性。对于简单的环境中，不需要复杂，多步骤的策略来学习和探索，在较短的周期内就可以获得频繁的反馈，cross-entropy方法通常效果比较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The taxonomy of RL methods\n",
    "\n",
    "cross-entropy 方法是免模型(model-free)和基于政策(policy-based)的方法。\n",
    "\n",
    "* 免模型：意味着这种方法不会建立一个关于环境和奖励的模型，而是直接对现存的观测进行计算得到需要采取的动作。\n",
    "* 基于策略：基于策略的方法可以近似看作是代理的策略，即代理在每一步应该执行什么操作，策略可以用操作的概念分布表示\n",
    "\n",
    "所以cross-entropy方法是model-free，policy-based和on-policy的方法：\n",
    "* 不需要构建环境模型，直接告诉代理如何在下一步采取动作\n",
    "* 近似代理的策略\n",
    "* 需要从环境中获取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross-entropy method in pratice\n",
    "\n",
    "在代理的活动周期，其对应的经历可以用一个周期(episode)表示，每一个周期(episode)是代理在环境中获得的观测，采取的动作以及从采取的动作获得奖励的集合。 \n",
    "\n",
    "由于环境和代理采取动作的随机性，一些活动周期比另一些要好一些。cross-entropy方法的核心是将一些表现差的活动周期扔掉然后在表现好的活动周期上训练，所以，该方法的流程如下：\n",
    "* 使用现有的模型和环境运行N次周期\n",
    "* 计算每个周期的奖励，然后设定一个奖励边界，通常使用的是所有奖励分数的百分数，例如50%或者70%\n",
    "* 丢弃所有奖励分数低于奖励边界的周期\n",
    "* 在保留**精英**episode上进行训练，将观测作为输入，对应的动作作为想要的输出\n",
    "* 从第一步开始重复直到结果令人满意\n",
    "\n",
    "> 在上述步骤进行的过程中，神经网络学习如何重复进行动作来获得更大的奖励，然后将奖励分数变得越来越高。尽管这个算法很简单，但是它在基本的环境中运行效果很好，很容易实施，调整超参数的时候也非常稳定，让它成为一个理性的baseline。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross-entropy method on CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c08bec6e8349631744bf8012680f4ed9f87ad05429dee36fcc42750dcdea16a4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('rlbook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
